{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Usual Libraries\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import sklearn\r\n",
    "\r\n",
    "# Librosa (the mother of audio files)\r\n",
    "import librosa\r\n",
    "import librosa.display\r\n",
    "import IPython.display as ipd\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "import os\r\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "current_path = os.getcwd()\r\n",
    "current_path = os.getcwd()\r\n",
    "genre_music_path = os.path.join(current_path, 'genres_original')\r\n",
    "class_xls_path = os.path.join(current_path, 'class_xls')\r\n",
    "print(list(os.listdir(genre_music_path)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Importing 1 file\r\n",
    "audio_file = os.path.join(genre_music_path,'reggae','reggae.00036.wav')\r\n",
    "y, sr = librosa.load(audio_file)\r\n",
    "\r\n",
    "print('y:', y, '\\n')\r\n",
    "print('y shape:', np.shape(y), '\\n')\r\n",
    "print('Sample Rate (KHz):', sr, '\\n')\r\n",
    "\r\n",
    "# Verify length of the audio\r\n",
    "print('Check Len of Audio:', 661794/22050)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y: [0.02072144 0.04492188 0.05422974 ... 0.06912231 0.08303833 0.08572388] \n",
      "\n",
      "y shape: (661794,) \n",
      "\n",
      "Sample Rate (KHz): 22050 \n",
      "\n",
      "Check Len of Audio: 30.013333333333332\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "from codeFeature.feature_extractor import *\r\n",
    "import re\r\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def GetGenre(genre_music_path):\r\n",
    "    label_names = [item for item in os.listdir(\r\n",
    "        genre_music_path) if os.path.isdir(os.path.join(genre_music_path, item))]\r\n",
    "    nb_train_samples = sum([len(files) for _, _, files in os.walk(genre_music_path)])\r\n",
    "    \r\n",
    "    return label_names, nb_train_samples\r\n",
    "genres, nb_train_samples = GetGenre(genre_music_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "dir_trainfolder = \"./gtzan/_train\"\r\n",
    "dir_devfolder = \"./gtzan/_validation\"\r\n",
    "dir_testfolder = \"./gtzan/_test\"\r\n",
    "dir_all_files = \"./gtzan\"\r\n",
    "\r\n",
    "train_X_preprocessed_data = \"./gtzan/data_train_input.npy\"\r\n",
    "train_Y_preprocessed_data = \"./gtzan/data_train_target.npy\"\r\n",
    "dev_X_preprocessed_data = \"./gtzan/data_validation_input.npy\"\r\n",
    "dev_Y_preprocessed_data = \"./gtzan/data_validation_target.npy\"\r\n",
    "test_X_preprocessed_data = \"./gtzan/data_test_input.npy\"\r\n",
    "test_Y_preprocessed_data = \"./gtzan/data_test_target.npy\"\r\n",
    "\r\n",
    "train_X = train_Y = None\r\n",
    "dev_X = dev_Y = None\r\n",
    "test_X = test_Y = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "hop_length = 512\r\n",
    "\r\n",
    "timeseries_length_list = []\r\n",
    "\r\n",
    "all_files_list = []\r\n",
    "\r\n",
    "# compute minimum timeseries length, slow to compute, caching pre-computed value of 1290\r\n",
    "# precompute_min_timeseries_len()\r\n",
    "# print(\"min(timeseries_length_list) ==\" + str(min(timeseries_length_list)))\r\n",
    "# timeseries_length = min(timeseries_length_list)\r\n",
    "\r\n",
    "timeseries_length = (\r\n",
    "    128\r\n",
    ")   # sequence length == 128, default fftsize == 2048 & hop == 512 @ SR of 22050\r\n",
    "#  equals 128 overlapped windows that cover approx ~3.065 seconds of audio, which is a bit small!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "def path_leaf(path):\r\n",
    "    head, tail = ntpath.split(path)\r\n",
    "    return tail or ntpath.basename(head)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def extract_audio_features(list_of_audiofiles):\r\n",
    "\r\n",
    "    data = np.zeros(\r\n",
    "        (len(list_of_audiofiles), timeseries_length, 33), dtype=np.float64\r\n",
    "    )\r\n",
    "    target = []\r\n",
    "\r\n",
    "    for i, file in enumerate(list_of_audiofiles):\r\n",
    "        file_name = path_leaf(file)\r\n",
    "        \r\n",
    "        splits = re.split(\"[ .]\", file_name)\r\n",
    "        # print(splits)\r\n",
    "        # genre = re.split(\"[ /]\", splits[1])[3]\r\n",
    "        genre = splits[0]\r\n",
    "        target.append(genre)\r\n",
    "        if FileCheck(file)!=1:\r\n",
    "            continue\r\n",
    "        y, sr = librosa.load(file)\r\n",
    "        mfcc = librosa.feature.mfcc(\r\n",
    "            y=y, sr=sr, hop_length=hop_length, n_mfcc=13\r\n",
    "        )\r\n",
    "        spectral_center = librosa.feature.spectral_centroid(\r\n",
    "            y=y, sr=sr, hop_length=hop_length\r\n",
    "        )\r\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=hop_length)\r\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(\r\n",
    "            y=y, sr=sr, hop_length=hop_length\r\n",
    "        )\r\n",
    "\r\n",
    "        data[i, :, 0:13] = mfcc.T[0:timeseries_length, :]\r\n",
    "        data[i, :, 13:14] = spectral_center.T[0:timeseries_length, :]\r\n",
    "        data[i, :, 14:26] = chroma.T[0:timeseries_length, :]\r\n",
    "        data[i, :, 26:33] = spectral_contrast.T[0:timeseries_length, :]\r\n",
    "\r\n",
    "        # print(\r\n",
    "        #     \"Extracted features audio track %i of %i.\"\r\n",
    "        #     % (i + 1, len(list_of_audiofiles))\r\n",
    "        # )\r\n",
    "\r\n",
    "    return data, np.expand_dims(np.asarray(target), axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "def FileCheck(fn):\r\n",
    "    try:\r\n",
    "        librosa.load(fn, mono=True, duration=30)\r\n",
    "        return 1\r\n",
    "    except:\r\n",
    "        print (f\"Error: File {fn} does not appear to exist.\")\r\n",
    "        return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "def getListOfFiles(dirName):\r\n",
    "    # create a list of file and sub directories \r\n",
    "    # names in the given directory \r\n",
    "    listOfFile = os.listdir(dirName)\r\n",
    "    allFiles = list()\r\n",
    "    # Iterate over all the entries\r\n",
    "    for entry in listOfFile:\r\n",
    "        # Create full path\r\n",
    "        fullPath = os.path.join(dirName, entry)\r\n",
    "        # If entry is a directory then get the list of files in this directory \r\n",
    "        if os.path.isdir(fullPath):\r\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\r\n",
    "        else:\r\n",
    "            allFiles.append(fullPath)\r\n",
    "                \r\n",
    "    return allFiles"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# [files for _, _, files in os.walk(genre_music_path)]\r\n",
    "list_of_audiofiles = [files for files in getListOfFiles(genre_music_path)]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "data = extract_audio_features(list_of_audiofiles)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error: File d:\\testRGB\\MusicClasiffication\\genres_original\\jazz\\jazz.00054.wav does not appear to exist.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "X, y = data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "X = np.array(X, dtype = float)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "X.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 128, 33)"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "X_train.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(750, 128, 33)"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# Preprocessing\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\r\n",
    "#Keras\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import LSTM\r\n",
    "from tensorflow.keras.layers import Dense\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "from tensorflow.keras.models import model_from_json"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "model = Sequential()\r\n",
    "model.add(LSTM(units=128, dropout=0.05, recurrent_dropout=0.35, return_sequences=True, input_shape=input_shape))\r\n",
    "model.add(LSTM(units=32,  dropout=0.05, recurrent_dropout=0.35, return_sequences=False))\r\n",
    "model.add(Dense(units=len(genres), activation=\"softmax\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "opt = Adam()\r\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\r\n",
    "# model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "X_train.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(750, 128, 33)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "def one_hot(Y_genre_strings):\r\n",
    "        y_one_hot = np.zeros((Y_genre_strings.shape[0], len(genres)))\r\n",
    "        for i, genre_string in enumerate(Y_genre_strings):\r\n",
    "            index = genres.index(genre_string)\r\n",
    "            y_one_hot[i, index] = 1\r\n",
    "        return y_one_hot"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "ytest_train = one_hot(y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "ytes_test = one_hot(y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "batch_size = 100  # num of training examples per minibatch\r\n",
    "num_epochs = 100\r\n",
    "model.fit(\r\n",
    "    X_train,\r\n",
    "    ytest_train,\r\n",
    "    validation_data=(X_test, ytes_test),\r\n",
    "    batch_size=batch_size,\r\n",
    "    epochs=num_epochs,\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5120 - accuracy: 0.8093 - val_loss: 1.8931 - val_accuracy: 0.4240\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4889 - accuracy: 0.8307 - val_loss: 2.0067 - val_accuracy: 0.4360\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4949 - accuracy: 0.8267 - val_loss: 1.9444 - val_accuracy: 0.4320\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5133 - accuracy: 0.8133 - val_loss: 1.8912 - val_accuracy: 0.4320\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4942 - accuracy: 0.8293 - val_loss: 1.8854 - val_accuracy: 0.4840\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4754 - accuracy: 0.8347 - val_loss: 1.8405 - val_accuracy: 0.4680\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5028 - accuracy: 0.8440 - val_loss: 1.9606 - val_accuracy: 0.4400\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5321 - accuracy: 0.8213 - val_loss: 1.9238 - val_accuracy: 0.4480\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5162 - accuracy: 0.8307 - val_loss: 2.0615 - val_accuracy: 0.4200\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4734 - accuracy: 0.8507 - val_loss: 1.9066 - val_accuracy: 0.4520\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4905 - accuracy: 0.8333 - val_loss: 2.1052 - val_accuracy: 0.3960\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5108 - accuracy: 0.8240 - val_loss: 2.1291 - val_accuracy: 0.4000\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4909 - accuracy: 0.8240 - val_loss: 2.0796 - val_accuracy: 0.4000\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4917 - accuracy: 0.8347 - val_loss: 2.0042 - val_accuracy: 0.4520\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4450 - accuracy: 0.8507 - val_loss: 1.8927 - val_accuracy: 0.4640\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4674 - accuracy: 0.8467 - val_loss: 1.9868 - val_accuracy: 0.4400\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4756 - accuracy: 0.8320 - val_loss: 1.9196 - val_accuracy: 0.4480\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4591 - accuracy: 0.8453 - val_loss: 1.9632 - val_accuracy: 0.4480\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4731 - accuracy: 0.8280 - val_loss: 1.8794 - val_accuracy: 0.4360\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4956 - accuracy: 0.8187 - val_loss: 1.8573 - val_accuracy: 0.4640\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4648 - accuracy: 0.8320 - val_loss: 1.9333 - val_accuracy: 0.4360\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4666 - accuracy: 0.8587 - val_loss: 1.9357 - val_accuracy: 0.4440\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4163 - accuracy: 0.8613 - val_loss: 1.9376 - val_accuracy: 0.4480\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.4184 - accuracy: 0.8520 - val_loss: 1.9211 - val_accuracy: 0.4560\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4500 - accuracy: 0.8520 - val_loss: 2.0099 - val_accuracy: 0.4520\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4578 - accuracy: 0.8320 - val_loss: 2.0198 - val_accuracy: 0.4320\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4796 - accuracy: 0.8253 - val_loss: 2.0456 - val_accuracy: 0.4160\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5056 - accuracy: 0.8200 - val_loss: 1.9735 - val_accuracy: 0.4600\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4907 - accuracy: 0.8240 - val_loss: 1.9880 - val_accuracy: 0.4240\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4756 - accuracy: 0.8373 - val_loss: 1.8698 - val_accuracy: 0.4800\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4605 - accuracy: 0.8507 - val_loss: 1.9737 - val_accuracy: 0.4160\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4433 - accuracy: 0.8600 - val_loss: 2.1175 - val_accuracy: 0.4160\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5065 - accuracy: 0.8427 - val_loss: 1.9774 - val_accuracy: 0.4440\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4948 - accuracy: 0.8253 - val_loss: 2.0506 - val_accuracy: 0.3880\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4738 - accuracy: 0.8467 - val_loss: 1.8636 - val_accuracy: 0.4440\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4768 - accuracy: 0.8453 - val_loss: 1.8774 - val_accuracy: 0.4400\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4633 - accuracy: 0.8373 - val_loss: 1.8824 - val_accuracy: 0.4280\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4507 - accuracy: 0.8373 - val_loss: 1.9048 - val_accuracy: 0.4400\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4472 - accuracy: 0.8600 - val_loss: 1.9434 - val_accuracy: 0.4560\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4275 - accuracy: 0.8533 - val_loss: 2.0758 - val_accuracy: 0.4200\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6641 - accuracy: 0.7653 - val_loss: 2.1054 - val_accuracy: 0.3800\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5876 - accuracy: 0.7907 - val_loss: 1.8409 - val_accuracy: 0.4640\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6260 - accuracy: 0.7827 - val_loss: 1.8257 - val_accuracy: 0.4840\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5422 - accuracy: 0.8173 - val_loss: 1.9416 - val_accuracy: 0.4520\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4804 - accuracy: 0.8467 - val_loss: 1.8456 - val_accuracy: 0.4480\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4886 - accuracy: 0.8187 - val_loss: 2.0344 - val_accuracy: 0.4240\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4966 - accuracy: 0.8280 - val_loss: 2.0072 - val_accuracy: 0.4240\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4942 - accuracy: 0.8253 - val_loss: 1.9624 - val_accuracy: 0.4160\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4650 - accuracy: 0.8453 - val_loss: 1.9031 - val_accuracy: 0.4520\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4232 - accuracy: 0.8653 - val_loss: 1.8930 - val_accuracy: 0.4320\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4367 - accuracy: 0.8573 - val_loss: 1.9400 - val_accuracy: 0.4320\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3925 - accuracy: 0.8667 - val_loss: 1.8383 - val_accuracy: 0.4600\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4017 - accuracy: 0.8600 - val_loss: 1.9068 - val_accuracy: 0.4520\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4015 - accuracy: 0.8600 - val_loss: 1.9846 - val_accuracy: 0.4280\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4031 - accuracy: 0.8640 - val_loss: 1.9399 - val_accuracy: 0.4320\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4366 - accuracy: 0.8573 - val_loss: 1.9370 - val_accuracy: 0.4440\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4309 - accuracy: 0.8613 - val_loss: 1.9200 - val_accuracy: 0.4400\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4073 - accuracy: 0.8760 - val_loss: 1.8602 - val_accuracy: 0.4480\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4490 - accuracy: 0.8307 - val_loss: 2.0088 - val_accuracy: 0.4400\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4367 - accuracy: 0.8640 - val_loss: 1.9456 - val_accuracy: 0.4560\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4466 - accuracy: 0.8520 - val_loss: 1.9811 - val_accuracy: 0.4360\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4114 - accuracy: 0.8653 - val_loss: 1.8713 - val_accuracy: 0.4520\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4379 - accuracy: 0.8427 - val_loss: 1.9908 - val_accuracy: 0.4360\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4066 - accuracy: 0.8613 - val_loss: 1.9506 - val_accuracy: 0.4560\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3894 - accuracy: 0.8773 - val_loss: 2.0354 - val_accuracy: 0.4640\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4004 - accuracy: 0.8600 - val_loss: 1.9797 - val_accuracy: 0.4200\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3866 - accuracy: 0.8693 - val_loss: 2.0699 - val_accuracy: 0.4240\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3866 - accuracy: 0.8680 - val_loss: 2.0640 - val_accuracy: 0.4200\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3766 - accuracy: 0.8707 - val_loss: 2.0617 - val_accuracy: 0.4440\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3865 - accuracy: 0.8667 - val_loss: 2.0020 - val_accuracy: 0.4240\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4193 - accuracy: 0.8507 - val_loss: 2.1149 - val_accuracy: 0.4320\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.4583 - accuracy: 0.8413 - val_loss: 2.0298 - val_accuracy: 0.4080\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4050 - accuracy: 0.8773 - val_loss: 1.9413 - val_accuracy: 0.4720\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4142 - accuracy: 0.8640 - val_loss: 1.9320 - val_accuracy: 0.4640\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3621 - accuracy: 0.8707 - val_loss: 2.1415 - val_accuracy: 0.4320\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3683 - accuracy: 0.8827 - val_loss: 1.9754 - val_accuracy: 0.4680\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3431 - accuracy: 0.8867 - val_loss: 1.9837 - val_accuracy: 0.4680\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3592 - accuracy: 0.8813 - val_loss: 2.0074 - val_accuracy: 0.4680\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3585 - accuracy: 0.8827 - val_loss: 2.1126 - val_accuracy: 0.4160\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.3583 - accuracy: 0.8733 - val_loss: 1.9348 - val_accuracy: 0.4600\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3612 - accuracy: 0.8880 - val_loss: 1.9493 - val_accuracy: 0.4600\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.4299 - accuracy: 0.8533 - val_loss: 1.9768 - val_accuracy: 0.4560\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5044 - accuracy: 0.8293 - val_loss: 2.0996 - val_accuracy: 0.4480\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3626 - accuracy: 0.8813 - val_loss: 1.9634 - val_accuracy: 0.4480\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3835 - accuracy: 0.8613 - val_loss: 1.9990 - val_accuracy: 0.4680\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3389 - accuracy: 0.8920 - val_loss: 1.9904 - val_accuracy: 0.4440\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3221 - accuracy: 0.8893 - val_loss: 2.0226 - val_accuracy: 0.4480\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3344 - accuracy: 0.8907 - val_loss: 1.9619 - val_accuracy: 0.4480\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.3158 - accuracy: 0.8907 - val_loss: 2.1219 - val_accuracy: 0.4480\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.3342 - accuracy: 0.8867 - val_loss: 1.9862 - val_accuracy: 0.4720\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3507 - accuracy: 0.8880 - val_loss: 2.1678 - val_accuracy: 0.4640\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3365 - accuracy: 0.8693 - val_loss: 2.0775 - val_accuracy: 0.4120\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3847 - accuracy: 0.8680 - val_loss: 2.0976 - val_accuracy: 0.4120\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3699 - accuracy: 0.8747 - val_loss: 1.8840 - val_accuracy: 0.4560\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3322 - accuracy: 0.8827 - val_loss: 2.0277 - val_accuracy: 0.4360\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3177 - accuracy: 0.8893 - val_loss: 2.1777 - val_accuracy: 0.4160\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3639 - accuracy: 0.8787 - val_loss: 1.9050 - val_accuracy: 0.4920\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.3428 - accuracy: 0.8787 - val_loss: 2.0016 - val_accuracy: 0.4880\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.3537 - accuracy: 0.8773 - val_loss: 2.0207 - val_accuracy: 0.4600\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.3861 - accuracy: 0.8747 - val_loss: 1.9772 - val_accuracy: 0.4400\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b8156a6ac0>"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# Creates a HDF5 file 'lstm_genre_classifier.h5'\r\n",
    "model_filename = \"./gtzan/model_weights.h5\"\r\n",
    "print(\"\\nSaving model: \" + model_filename)\r\n",
    "model.save(model_filename)\r\n",
    "# Creates a json file\r\n",
    "# print(\"creating .json file....\")\r\n",
    "model_json = model.to_json()\r\n",
    "f = \"./gtzan/model.json\"\r\n",
    "#save the model architecture to JSON file\r\n",
    "with open(f, 'w') as json_file:\r\n",
    "    json_file.write(model_json)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Saving model: ./gtzan/model_weights.h5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "# X_train, X_test, y_train, y_test "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "train_X_preprocessed_data = \"./gtzan/data_train_input.npy\"\r\n",
    "train_Y_preprocessed_data = \"./gtzan/data_train_target.npy\"\r\n",
    "dev_X_preprocessed_data = \"./gtzan/data_validation_input.npy\"\r\n",
    "dev_Y_preprocessed_data = \"./gtzan/data_validation_target.npy\"\r\n",
    "test_X_preprocessed_data = \"./gtzan/data_test_input.npy\"\r\n",
    "test_Y_preprocessed_data = \"./gtzan/data_test_target.npy\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "with open(train_X_preprocessed_data, \"wb\") as f:\r\n",
    "            np.save(f, X_train)\r\n",
    "with open(train_Y_preprocessed_data, \"wb\") as f:\r\n",
    "            np.save(f, one_hot(y_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "with open(test_X_preprocessed_data, \"wb\") as f:\r\n",
    "            np.save(f, X_test)\r\n",
    "with open(test_Y_preprocessed_data, \"wb\") as f:\r\n",
    "            np.save(f, one_hot(y_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "print(\"\\nValidating ...\")\r\n",
    "score, accuracy = model.evaluate(\r\n",
    "    X_test, one_hot(y_test), batch_size=batch_size, verbose=1\r\n",
    ")\r\n",
    "print(\"Dev loss:  \", score)\r\n",
    "print(\"Dev accuracy:  \", accuracy)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Validating ...\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 1.9772 - accuracy: 0.4400\n",
      "Dev loss:   1.9771649837493896\n",
      "Dev accuracy:   0.4399999976158142\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "def load_model(model_path, weights_path):\r\n",
    "    \"Load the trained LSTM model from directory for genre classification\"\r\n",
    "    with open(model_path, \"r\") as model_file:\r\n",
    "        trained_model = model_from_json(model_file.read())\r\n",
    "    trained_model.load_weights(weights_path)\r\n",
    "    trained_model.compile(\r\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\r\n",
    "    )\r\n",
    "    return trained_model\r\n",
    "\r\n",
    "\r\n",
    "def extract_audio_features(file):\r\n",
    "    \"Extract audio features from an audio file for genre classification\"\r\n",
    "    timeseries_length = 128\r\n",
    "    features = np.zeros((1, timeseries_length, 33), dtype=np.float64)\r\n",
    "\r\n",
    "    y, sr = librosa.load(file)\r\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=512, n_mfcc=13)\r\n",
    "    spectral_center = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=512)\r\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=512)\r\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=512)\r\n",
    "\r\n",
    "    features[0, :, 0:13] = mfcc.T[0:timeseries_length, :]\r\n",
    "    features[0, :, 13:14] = spectral_center.T[0:timeseries_length, :]\r\n",
    "    features[0, :, 14:26] = chroma.T[0:timeseries_length, :]\r\n",
    "    features[0, :, 26:33] = spectral_contrast.T[0:timeseries_length, :]\r\n",
    "    return features\r\n",
    "\r\n",
    "\r\n",
    "def get_genre(model, music_path):\r\n",
    "    \"Predict genre of music using a trained model\"\r\n",
    "    prediction = model.predict(extract_audio_features(music_path))\r\n",
    "    predict_genre = genres[np.argmax(prediction)]\r\n",
    "    return predict_genre\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "if __name__ == \"__main__\":\r\n",
    "    PATH = \"./genres_original/reggae/reggae.00015.wav\"\r\n",
    "    MODEL = load_model(\"./gtzan/model.json\", \"./gtzan/model_weights.h5\")\r\n",
    "    GENRE = get_genre(MODEL, PATH)\r\n",
    "    print(\"Model predict: {}\".format(GENRE))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model predict: reggae\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "23300995598eec4bcf6bd89cf02d1c3675e8b2616661418dbbf5580aa901878d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}