{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# Usual Libraries\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import sklearn\r\n",
    "\r\n",
    "# Librosa (the mother of audio files)\r\n",
    "import librosa\r\n",
    "import librosa.display\r\n",
    "import IPython.display as ipd\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "import os\r\n",
    "from pathlib import Path"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "current_path = os.getcwd()\r\n",
    "current_path = os.getcwd()\r\n",
    "genre_music_path = os.path.join(current_path, 'genres_original')\r\n",
    "class_xls_path = os.path.join(current_path, 'class_xls')\r\n",
    "print(list(os.listdir(genre_music_path)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Importing 1 file\r\n",
    "audio_file = os.path.join(genre_music_path,'reggae','reggae.00036.wav')\r\n",
    "y, sr = librosa.load(audio_file)\r\n",
    "\r\n",
    "print('y:', y, '\\n')\r\n",
    "print('y shape:', np.shape(y), '\\n')\r\n",
    "print('Sample Rate (KHz):', sr, '\\n')\r\n",
    "\r\n",
    "# Verify length of the audio\r\n",
    "print('Check Len of Audio:', 661794/22050)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y: [0.02072144 0.04492188 0.05422974 ... 0.06912231 0.08303833 0.08572388] \n",
      "\n",
      "y shape: (661794,) \n",
      "\n",
      "Sample Rate (KHz): 22050 \n",
      "\n",
      "Check Len of Audio: 30.013333333333332\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from codeFeature.feature_extractor import *\r\n",
    "import re\r\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def GetGenre(genre_music_path):\r\n",
    "    label_names = [item for item in os.listdir(\r\n",
    "        genre_music_path) if os.path.isdir(os.path.join(genre_music_path, item))]\r\n",
    "    nb_train_samples = sum([len(files) for _, _, files in os.walk(genre_music_path)])\r\n",
    "    \r\n",
    "    return label_names, nb_train_samples\r\n",
    "genres, nb_train_samples = GetGenre(genre_music_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "dir_trainfolder = \"./gtzan/_train\"\r\n",
    "dir_devfolder = \"./gtzan/_validation\"\r\n",
    "dir_testfolder = \"./gtzan/_test\"\r\n",
    "dir_all_files = \"./gtzan\"\r\n",
    "\r\n",
    "train_X_preprocessed_data = \"./gtzan/data_train_input.npy\"\r\n",
    "train_Y_preprocessed_data = \"./gtzan/data_train_target.npy\"\r\n",
    "dev_X_preprocessed_data = \"./gtzan/data_validation_input.npy\"\r\n",
    "dev_Y_preprocessed_data = \"./gtzan/data_validation_target.npy\"\r\n",
    "test_X_preprocessed_data = \"./gtzan/data_test_input.npy\"\r\n",
    "test_Y_preprocessed_data = \"./gtzan/data_test_target.npy\"\r\n",
    "\r\n",
    "train_X = train_Y = None\r\n",
    "dev_X = dev_Y = None\r\n",
    "test_X = test_Y = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "hop_length = 512\r\n",
    "\r\n",
    "timeseries_length_list = []\r\n",
    "\r\n",
    "all_files_list = []\r\n",
    "\r\n",
    "# compute minimum timeseries length, slow to compute, caching pre-computed value of 1290\r\n",
    "# precompute_min_timeseries_len()\r\n",
    "# print(\"min(timeseries_length_list) ==\" + str(min(timeseries_length_list)))\r\n",
    "# timeseries_length = min(timeseries_length_list)\r\n",
    "\r\n",
    "timeseries_length = (\r\n",
    "    128\r\n",
    ")   # sequence length == 128, default fftsize == 2048 & hop == 512 @ SR of 22050\r\n",
    "#  equals 128 overlapped windows that cover approx ~3.065 seconds of audio, which is a bit small!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def path_leaf(path):\r\n",
    "    head, tail = ntpath.split(path)\r\n",
    "    return tail or ntpath.basename(head)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def extract_audio_features(list_of_audiofiles):\r\n",
    "\r\n",
    "    data = np.zeros(\r\n",
    "        (len(list_of_audiofiles), timeseries_length, 33), dtype=np.float64\r\n",
    "    )\r\n",
    "    target = []\r\n",
    "\r\n",
    "    for i, file in enumerate(list_of_audiofiles):\r\n",
    "        file_name = path_leaf(file)\r\n",
    "        \r\n",
    "        splits = re.split(\"[ .]\", file_name)\r\n",
    "        # print(splits)\r\n",
    "        # genre = re.split(\"[ /]\", splits[1])[3]\r\n",
    "        genre = splits[0]\r\n",
    "        target.append(genre)\r\n",
    "        if FileCheck(file)!=1:\r\n",
    "            continue\r\n",
    "        y, sr = librosa.load(file)\r\n",
    "        mfcc = librosa.feature.mfcc(\r\n",
    "            y=y, sr=sr, hop_length=hop_length, n_mfcc=13\r\n",
    "        )\r\n",
    "        spectral_center = librosa.feature.spectral_centroid(\r\n",
    "            y=y, sr=sr, hop_length=hop_length\r\n",
    "        )\r\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=hop_length)\r\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(\r\n",
    "            y=y, sr=sr, hop_length=hop_length\r\n",
    "        )\r\n",
    "\r\n",
    "        data[i, :, 0:13] = mfcc.T[0:timeseries_length, :]\r\n",
    "        data[i, :, 13:14] = spectral_center.T[0:timeseries_length, :]\r\n",
    "        data[i, :, 14:26] = chroma.T[0:timeseries_length, :]\r\n",
    "        data[i, :, 26:33] = spectral_contrast.T[0:timeseries_length, :]\r\n",
    "\r\n",
    "        # print(\r\n",
    "        #     \"Extracted features audio track %i of %i.\"\r\n",
    "        #     % (i + 1, len(list_of_audiofiles))\r\n",
    "        # )\r\n",
    "\r\n",
    "    return data, np.expand_dims(np.asarray(target), axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def FileCheck(fn):\r\n",
    "    try:\r\n",
    "        librosa.load(fn, mono=True, duration=30)\r\n",
    "        return 1\r\n",
    "    except:\r\n",
    "        print (f\"Error: File {fn} does not appear to exist.\")\r\n",
    "        return 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def getListOfFiles(dirName):\r\n",
    "    # create a list of file and sub directories \r\n",
    "    # names in the given directory \r\n",
    "    listOfFile = os.listdir(dirName)\r\n",
    "    allFiles = list()\r\n",
    "    # Iterate over all the entries\r\n",
    "    for entry in listOfFile:\r\n",
    "        # Create full path\r\n",
    "        fullPath = os.path.join(dirName, entry)\r\n",
    "        # If entry is a directory then get the list of files in this directory \r\n",
    "        if os.path.isdir(fullPath):\r\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\r\n",
    "        else:\r\n",
    "            allFiles.append(fullPath)\r\n",
    "                \r\n",
    "    return allFiles"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# [files for _, _, files in os.walk(genre_music_path)]\r\n",
    "list_of_audiofiles = [files for files in getListOfFiles(genre_music_path)]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "data = extract_audio_features(list_of_audiofiles)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error: File d:\\testRGB\\MusicClasiffication\\genres_original\\jazz\\jazz.00054.wav does not appear to exist.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "X, y = data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "X = np.array(X, dtype = float)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "X.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 128, 33)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "X_train.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(800, 128, 33)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "input_shape = (X_train.shape[1], X_train.shape[2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# Preprocessing\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\r\n",
    "#Keras\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import LSTM\r\n",
    "from tensorflow.keras.layers import Dense\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "from tensorflow.keras.models import model_from_json"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "model = Sequential()\r\n",
    "model.add(LSTM(units=128, dropout=0.05, recurrent_dropout=0.35, return_sequences=True, input_shape=input_shape))\r\n",
    "model.add(LSTM(units=32,  dropout=0.05, recurrent_dropout=0.35, return_sequences=False))\r\n",
    "model.add(Dense(units=len(genres), activation=\"softmax\"))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "opt = Adam()\r\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\r\n",
    "# model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "X_train.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(800, 128, 33)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def one_hot(Y_genre_strings):\r\n",
    "        y_one_hot = np.zeros((Y_genre_strings.shape[0], len(genres)))\r\n",
    "        for i, genre_string in enumerate(Y_genre_strings):\r\n",
    "            index = genres.index(genre_string)\r\n",
    "            y_one_hot[i, index] = 1\r\n",
    "        return y_one_hot"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "ytesy = one_hot(y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "ytesy"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "batch_size = 35  # num of training examples per minibatch\r\n",
    "num_epochs = 30\r\n",
    "model.fit(\r\n",
    "    X_train,\r\n",
    "    ytesy,\r\n",
    "    batch_size=batch_size,\r\n",
    "    epochs=num_epochs,\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "23/23 [==============================] - 33s 1s/step - loss: 1.8309 - accuracy: 0.3500\n",
      "Epoch 2/30\n",
      "23/23 [==============================] - 34s 1s/step - loss: 1.7924 - accuracy: 0.3550\n",
      "Epoch 3/30\n",
      "23/23 [==============================] - 33s 1s/step - loss: 1.7547 - accuracy: 0.3700\n",
      "Epoch 4/30\n",
      "23/23 [==============================] - 32s 1s/step - loss: 1.7535 - accuracy: 0.3525\n",
      "Epoch 5/30\n",
      "23/23 [==============================] - 34s 1s/step - loss: 1.7241 - accuracy: 0.3600\n",
      "Epoch 6/30\n",
      "23/23 [==============================] - 34s 1s/step - loss: 1.6782 - accuracy: 0.3900\n",
      "Epoch 7/30\n",
      "23/23 [==============================] - 33s 1s/step - loss: 1.6326 - accuracy: 0.4000\n",
      "Epoch 8/30\n",
      "23/23 [==============================] - 33s 1s/step - loss: 1.6512 - accuracy: 0.3913\n",
      "Epoch 9/30\n",
      "23/23 [==============================] - 32s 1s/step - loss: 1.7152 - accuracy: 0.3675\n",
      "Epoch 10/30\n",
      "23/23 [==============================] - 32s 1s/step - loss: 1.6373 - accuracy: 0.3950\n",
      "Epoch 11/30\n",
      "23/23 [==============================] - 32s 1s/step - loss: 1.6205 - accuracy: 0.4162\n",
      "Epoch 12/30\n",
      "23/23 [==============================] - 33s 1s/step - loss: 1.6585 - accuracy: 0.3975\n",
      "Epoch 13/30\n",
      "23/23 [==============================] - 32s 1s/step - loss: 1.5757 - accuracy: 0.4313\n",
      "Epoch 14/30\n",
      "23/23 [==============================] - 33s 1s/step - loss: 1.5571 - accuracy: 0.4275\n",
      "Epoch 15/30\n",
      "23/23 [==============================] - 33s 1s/step - loss: 1.5318 - accuracy: 0.4400\n",
      "Epoch 16/30\n",
      "23/23 [==============================] - 33s 1s/step - loss: 1.4899 - accuracy: 0.4663\n",
      "Epoch 17/30\n",
      "23/23 [==============================] - 33s 1s/step - loss: 1.4954 - accuracy: 0.4663\n",
      "Epoch 18/30\n",
      "23/23 [==============================] - 33s 1s/step - loss: 1.4937 - accuracy: 0.4700\n",
      "Epoch 19/30\n",
      "23/23 [==============================] - 33s 1s/step - loss: 1.4841 - accuracy: 0.4588\n",
      "Epoch 20/30\n",
      "23/23 [==============================] - 33s 1s/step - loss: 1.4761 - accuracy: 0.4725\n",
      "Epoch 21/30\n",
      "23/23 [==============================] - 32s 1s/step - loss: 1.4152 - accuracy: 0.4725\n",
      "Epoch 22/30\n",
      "23/23 [==============================] - 33s 1s/step - loss: 1.4221 - accuracy: 0.4663\n",
      "Epoch 23/30\n",
      "23/23 [==============================] - 32s 1s/step - loss: 1.4244 - accuracy: 0.4575\n",
      "Epoch 24/30\n",
      "23/23 [==============================] - 31s 1s/step - loss: 1.4342 - accuracy: 0.4688\n",
      "Epoch 25/30\n",
      " 9/23 [==========>...................] - ETA: 19s - loss: 1.5527 - accuracy: 0.4317"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34844/3366990737.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m35\u001b[0m  \u001b[1;31m# num of training examples per minibatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m model.fit(\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mytesy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "# Creates a HDF5 file 'lstm_genre_classifier.h5'\r\n",
    "model_filename = \"./gtzan/model_weights.h5\"\r\n",
    "print(\"\\nSaving model: \" + model_filename)\r\n",
    "model.save(model_filename)\r\n",
    "# Creates a json file\r\n",
    "# print(\"creating .json file....\")\r\n",
    "model_json = model.to_json()\r\n",
    "f = \"./gtzan/model.json\"\r\n",
    "#save the model architecture to JSON file\r\n",
    "with open(f, 'w') as json_file:\r\n",
    "    json_file.write(model_json)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Saving model: ./gtzan/model_weights.h5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# X_train, X_test, y_train, y_test "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "train_X_preprocessed_data = \"./gtzan/data_train_input.npy\"\r\n",
    "train_Y_preprocessed_data = \"./gtzan/data_train_target.npy\"\r\n",
    "dev_X_preprocessed_data = \"./gtzan/data_validation_input.npy\"\r\n",
    "dev_Y_preprocessed_data = \"./gtzan/data_validation_target.npy\"\r\n",
    "test_X_preprocessed_data = \"./gtzan/data_test_input.npy\"\r\n",
    "test_Y_preprocessed_data = \"./gtzan/data_test_target.npy\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "with open(train_X_preprocessed_data, \"wb\") as f:\r\n",
    "            np.save(f, X_train)\r\n",
    "with open(train_Y_preprocessed_data, \"wb\") as f:\r\n",
    "            np.save(f, one_hot(y_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "with open(test_X_preprocessed_data, \"wb\") as f:\r\n",
    "            np.save(f, X_test)\r\n",
    "with open(test_Y_preprocessed_data, \"wb\") as f:\r\n",
    "            np.save(f, one_hot(y_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "print(\"\\nValidating ...\")\r\n",
    "score, accuracy = model.evaluate(\r\n",
    "    X_test, one_hot(y_test), batch_size=batch_size, verbose=1\r\n",
    ")\r\n",
    "print(\"Dev loss:  \", score)\r\n",
    "print(\"Dev accuracy:  \", accuracy)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Validating ...\n",
      "6/6 [==============================] - 2s 277ms/step - loss: 2.1246 - accuracy: 0.5250\n",
      "Dev loss:   2.124593496322632\n",
      "Dev accuracy:   0.5249999761581421\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "def load_model(model_path, weights_path):\r\n",
    "    \"Load the trained LSTM model from directory for genre classification\"\r\n",
    "    with open(model_path, \"r\") as model_file:\r\n",
    "        trained_model = model_from_json(model_file.read())\r\n",
    "    trained_model.load_weights(weights_path)\r\n",
    "    trained_model.compile(\r\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\r\n",
    "    )\r\n",
    "    return trained_model\r\n",
    "\r\n",
    "\r\n",
    "def extract_audio_features(file):\r\n",
    "    \"Extract audio features from an audio file for genre classification\"\r\n",
    "    timeseries_length = 128\r\n",
    "    features = np.zeros((1, timeseries_length, 33), dtype=np.float64)\r\n",
    "\r\n",
    "    y, sr = librosa.load(file)\r\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=512, n_mfcc=13)\r\n",
    "    spectral_center = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=512)\r\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=512)\r\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=512)\r\n",
    "\r\n",
    "    features[0, :, 0:13] = mfcc.T[0:timeseries_length, :]\r\n",
    "    features[0, :, 13:14] = spectral_center.T[0:timeseries_length, :]\r\n",
    "    features[0, :, 14:26] = chroma.T[0:timeseries_length, :]\r\n",
    "    features[0, :, 26:33] = spectral_contrast.T[0:timeseries_length, :]\r\n",
    "    return features\r\n",
    "\r\n",
    "\r\n",
    "def get_genre(model, music_path):\r\n",
    "    \"Predict genre of music using a trained model\"\r\n",
    "    prediction = model.predict(extract_audio_features(music_path))\r\n",
    "    predict_genre = genres[np.argmax(prediction)]\r\n",
    "    return predict_genre\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "if __name__ == \"__main__\":\r\n",
    "    PATH = \"./bensound-highoctane.mp3\"\r\n",
    "    MODEL = load_model(\"./gtzan/model.json\", \"./gtzan/model_weights.h5\")\r\n",
    "    GENRE = get_genre(MODEL, PATH)\r\n",
    "    print(\"Model predict: {}\".format(GENRE))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'GenreFeatureData' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34844/1799336879.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mPATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./bensound-highoctane.mp3\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mMODEL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./gtzan/model.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"./gtzan/model_weights.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mGENRE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_genre\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model predict: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGENRE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34844/3088751917.py\u001b[0m in \u001b[0;36mget_genre\u001b[1;34m(model, music_path)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;34m\"Predict genre of music using a trained model\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_audio_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmusic_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mpredict_genre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGenreFeatureData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenre_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpredict_genre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GenreFeatureData' is not defined"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "23300995598eec4bcf6bd89cf02d1c3675e8b2616661418dbbf5580aa901878d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}